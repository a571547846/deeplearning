{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "from utils import *  \n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#吴rnn第一周的作业二，任务是起一些新的恐龙名字。给的数据集是已有的恐龙名字的txt，训练时按字母，对每个字母，输出的下一个最可能\n",
    "#的字母是什么（在前面已经有的那些字母的前提下最可能的）。训练好后，在进行预测时，其实是造恐龙名字的过程，所以比如第一个字母是随便\n",
    "#的一个，第二个开始，输入到网络的字母就不是真的开发或测试集的X了，而是第一个字母输出的softmax概率向量里按概率随机选的一个（从里面）\n",
    "#随机选下个字母是a到z中哪个，或者是终止符（这里是\\n），然后直到预测到为换行符停止。\n",
    "#上面的随机选下一个输入的步骤叫做随机采样。\n",
    "data = open('dinos.txt', 'r').read()  \n",
    "data= data.lower()  \n",
    "chars = list(set(data))  \n",
    "data_size, vocab_size = len(data), len(chars)  \n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }  \n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }  \n",
    "print(ix_to_char)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Before updating the parameters, you will perform gradient clipping when needed to make sure t\n",
    "#hat your gradients are not \"exploding,\" meaning taking on overly large values.\n",
    "#梯度裁剪的函数\n",
    "### GRADED FUNCTION: clip  \n",
    "  \n",
    "def clip(gradients, maxValue):  \n",
    "    ''''' \n",
    "    Clips the gradients' values between minimum and maximum. \n",
    "     \n",
    "    Arguments: \n",
    "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\" \n",
    "    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue \n",
    "     \n",
    "    Returns:  \n",
    "    gradients -- a dictionary with the clipped gradients. \n",
    "    '''  \n",
    "      \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']  \n",
    "     \n",
    "    ### START CODE HERE ###  \n",
    "    # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines)  \n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:  \n",
    "        np.clip(gradient, -maxValue, maxValue, gradient)  \n",
    "    ### END CODE HERE ###  \n",
    "      \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}  \n",
    "      \n",
    "    return gradients  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#上面的函数的测试\n",
    "np.random.seed(3)  \n",
    "dWax = np.random.randn(5,3)*10  \n",
    "dWaa = np.random.randn(5,5)*10  \n",
    "dWya = np.random.randn(2,5)*10  \n",
    "db = np.random.randn(5,1)*10  \n",
    "dby = np.random.randn(2,1)*10  \n",
    "gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}  \n",
    "gradients = clip(gradients, 10)  \n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])  \n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])  \n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])  \n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])  \n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#按上个字母输入后，softmax层概率分布随机选出下个字母的采样函数\n",
    "# GRADED FUNCTION: sample  \n",
    "  \n",
    "def sample(parameters, char_to_ix, seed):  \n",
    "    \"\"\" \n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN \n",
    " \n",
    "    Arguments: \n",
    "    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b.  \n",
    "    char_to_ix -- python dictionary mapping each character to an index. \n",
    "    seed -- used for grading purposes. Do not worry about it. \n",
    " \n",
    "    Returns: \n",
    "    indices -- a list of length n containing the indices of the sampled characters. \n",
    "    \"\"\"  \n",
    "      \n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary  \n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']  \n",
    "    vocab_size = by.shape[0]  \n",
    "    n_a = Waa.shape[1]  \n",
    "      \n",
    "    ### START CODE HERE ###  \n",
    "    # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (≈1 line)  \n",
    "    x = np.zeros((vocab_size,1))  \n",
    "    # Step 1': Initialize a_prev as zeros (≈1 line)  \n",
    "    a_prev = np.zeros((n_a,1))  \n",
    "      \n",
    "    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line)  \n",
    "    indices = []  \n",
    "      \n",
    "    # Idx is a flag to detect a newline character, we initialize it to -1  \n",
    "    idx = -1   \n",
    "      \n",
    "    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append   \n",
    "    # its index to \"indices\". We'll stop if we reach 50 characters (which should be very unlikely with a well   \n",
    "    # trained model), which helps debugging and prevents entering an infinite loop.   \n",
    "    counter = 0  \n",
    "    newline_character = char_to_ix['\\n']  \n",
    "      \n",
    "    while (idx != newline_character and counter != 50):  \n",
    "          \n",
    "        # Step 2: Forward propagate x using the equations (1), (2) and (3)  \n",
    "        a = np.tanh(np.dot(Wax,x)+np.dot(Waa,a_prev) + b)  \n",
    "        z = np.dot(Wya, a) + by  \n",
    "        y = softmax(z)  \n",
    "          \n",
    "        # for grading purposes  \n",
    "        np.random.seed(counter+seed)   \n",
    "          \n",
    "        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y  \n",
    "        idx = np.random.choice(range(vocab_size), p = y.ravel())  \n",
    "  \n",
    "        # Append the index to \"indices\"  \n",
    "        indices.append(idx)  \n",
    "          \n",
    "        # Step 4: Overwrite the input character as the one corresponding to the sampled index.  \n",
    "        x = np.zeros((vocab_size,1))  \n",
    "        x[idx] = 1  \n",
    "          \n",
    "        # Update \"a_prev\" to be \"a\"  \n",
    "        a_prev = a  \n",
    "          \n",
    "        # for grading purposes  \n",
    "        seed += 1  \n",
    "        counter +=1  \n",
    "          \n",
    "    ### END CODE HERE ###  \n",
    "  \n",
    "    if (counter == 50):  \n",
    "        indices.append(char_to_ix['\\n'])  \n",
    "      \n",
    "    return indices  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#上面函数的测试\n",
    "np.random.seed(2)  \n",
    "_, n_a = 20, 100  \n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)  \n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)  \n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}  \n",
    "  \n",
    "  \n",
    "indices = sample(parameters, char_to_ix, 0)  \n",
    "print(\"Sampling:\")  \n",
    "print(\"list of sampled indices:\", indices)  \n",
    "print(\"list of sampled characters:\", [ix_to_char[i] for i in indices])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#先后向传播的函数在工具脚本中，这个函数由梯度下降算法进行优化（随机梯度下降：一次一个样本）\n",
    "# GRADED FUNCTION: optimize  \n",
    "  \n",
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):  \n",
    "    \"\"\" \n",
    "    Execute one step of the optimization to train the model. \n",
    "     \n",
    "    Arguments: \n",
    "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary. \n",
    "    Y -- list of integers, exactly the same as X but shifted one index to the left. \n",
    "    a_prev -- previous hidden state. \n",
    "    parameters -- python dictionary containing: \n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x) \n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a) \n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a) \n",
    "                        b --  Bias, numpy array of shape (n_a, 1) \n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1) \n",
    "    learning_rate -- learning rate for the model. \n",
    "     \n",
    "    Returns: \n",
    "    loss -- value of the loss function (cross-entropy) \n",
    "    gradients -- python dictionary containing: \n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x) \n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a) \n",
    "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a) \n",
    "                        db -- Gradients of bias vector, of shape (n_a, 1) \n",
    "                        dby -- Gradients of output bias vector, of shape (n_y, 1) \n",
    "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1) \n",
    "    \"\"\"  \n",
    "      \n",
    "    ### START CODE HERE ###  \n",
    "      \n",
    "    # Forward propagate through time (≈1 line)  \n",
    "    loss, cache = rnn_forward(X, Y, a_prev, parameters)  \n",
    "      \n",
    "    # Backpropagate through time (≈1 line)  \n",
    "    gradients, a = rnn_backward(X, Y, parameters, cache)  \n",
    "      \n",
    "    # Clip your gradients between -5 (min) and 5 (max) (≈1 line)  \n",
    "    gradients = clip(gradients, 5)  \n",
    "      \n",
    "    # Update parameters (≈1 line)  \n",
    "    parameters = update_parameters(parameters, gradients, learning_rate)  \n",
    "      \n",
    "    ### END CODE HERE ###  \n",
    "      \n",
    "    return loss, gradients, a[len(X)-1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#上面函数的测试\n",
    "np.random.seed(1)  \n",
    "vocab_size, n_a = 27, 100  \n",
    "a_prev = np.random.randn(n_a, 1)  \n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)  \n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)  \n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}  \n",
    "X = [12,3,5,11,22,3]  \n",
    "Y = [4,14,11,22,25, 26]  \n",
    "  \n",
    "loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)  \n",
    "print(\"Loss =\", loss)  \n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])  \n",
    "print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))  \n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])  \n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])  \n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])  \n",
    "print(\"a_last[4] =\", a_last[4])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#训练模型的函数\n",
    "# GRADED FUNCTION: model  \n",
    "  \n",
    "def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):  \n",
    "    \"\"\" \n",
    "    Trains the model and generates dinosaur names.  \n",
    "     \n",
    "    Arguments: \n",
    "    data -- text corpus \n",
    "    ix_to_char -- dictionary that maps the index to a character \n",
    "    char_to_ix -- dictionary that maps a character to an index \n",
    "    num_iterations -- number of iterations to train the model for \n",
    "    n_a -- number of units of the RNN cell \n",
    "    dino_names -- number of dinosaur names you want to sample at each iteration.  \n",
    "    vocab_size -- number of unique characters found in the text, size of the vocabulary \n",
    "     \n",
    "    Returns: \n",
    "    parameters -- learned parameters \n",
    "    \"\"\"  \n",
    "      \n",
    "    # Retrieve n_x and n_y from vocab_size  \n",
    "    n_x, n_y = vocab_size, vocab_size  \n",
    "      \n",
    "    # Initialize parameters  \n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)  \n",
    "      \n",
    "    # Initialize loss (this is required because we want to smooth our loss, don't worry about it)  \n",
    "    loss = get_initial_loss(vocab_size, dino_names)  \n",
    "      \n",
    "    # Build list of all dinosaur names (training examples).  \n",
    "    with open(\"dinos.txt\") as f:  \n",
    "        examples = f.readlines()  \n",
    "    examples = [x.lower().strip() for x in examples]  \n",
    "      \n",
    "    # Shuffle list of all dinosaur names  \n",
    "    np.random.seed(0)  \n",
    "    np.random.shuffle(examples)  \n",
    "      \n",
    "    # Initialize the hidden state of your LSTM  \n",
    "    a_prev = np.zeros((n_a, 1))  \n",
    "      \n",
    "    # Optimization loop  \n",
    "    for j in range(num_iterations):  \n",
    "          \n",
    "        ### START CODE HERE ###  \n",
    "          \n",
    "        # Use the hint above to define one training example (X,Y) (≈ 2 lines)  \n",
    "        index = j % len(examples)  \n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]]   \n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]  \n",
    "          \n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters  \n",
    "        # Choose a learning rate of 0.01  \n",
    "        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)  \n",
    "          \n",
    "        ### END CODE HERE ###  \n",
    "          \n",
    "        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.  \n",
    "        loss = smooth(loss, curr_loss)  \n",
    "  \n",
    "        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly  \n",
    "        if j % 2000 == 0:  \n",
    "              \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')  \n",
    "              \n",
    "            # The number of dinosaur names to print  \n",
    "            seed = 0  \n",
    "            for name in range(dino_names):  \n",
    "                  \n",
    "                # Sample indices and print them  \n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)  \n",
    "                print_sample(sampled_indices, ix_to_char)  \n",
    "                  \n",
    "                seed += 1  # To get the same result for grading purposed, increment the seed by one.   \n",
    "        \n",
    "            print('\\n')  \n",
    "          \n",
    "    return parameters  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
